# Imports de la bibliothÃ¨que standard
import os
import random
import re
import shutil
from collections import defaultdict

# Imports de bibliothÃ¨ques tierces
import cv2
import numpy as np
import torch
from PIL import Image
from sklearn.metrics.pairwise import cosine_similarity
from torchvision import models, transforms
from tqdm import tqdm

def charger_mobilenet(device):

    print("CUDA disponible :", torch.cuda.is_available())
    print("Nom du GPU :", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "Aucun GPU")
    # Charger MobileNetV2 sans la derniÃ¨re couche de classification
    mobilenet = models.mobilenet_v2(pretrained=True)
    mobilenet.classifier = torch.nn.Identity()
    mobilenet.eval()
    return mobilenet.to(device)

# --------------------------------------------------
# ğŸ§¹ 1. Nettoyage des fichiers invalides
# --------------------------------------------------

def nettoyer_dataset(dataset_dir, classes_folders):
    err_dir = "../dataset_errone"
    os.makedirs(err_dir, exist_ok=True)

    def contient_caracteres_speciaux(texte):
        return bool(re.search(r'[^a-zA-Z0-9_\-\\/\. ]', texte))

    def extension_valide(fichier):
        return os.path.splitext(fichier)[-1].lower() in ['.jpg', '.jpeg', '.png']

    for nom in classes_folders:
            print(f"ğŸ” VÃ©rification du dossier : {nom}")
            full_class_path = os.path.join(dataset_dir, nom)  # Chemin complet
            for fichier in os.listdir(full_class_path):
                chemin_complet = os.path.join(nom, fichier)
                
                if os.path.isfile(chemin_complet):
                    # ğŸ”¸ VÃ©rification du nom de fichier et de l'extension
                    if contient_caracteres_speciaux(fichier) or not extension_valide(fichier):
                        dest = os.path.join(err_dir, nom)
                        os.makedirs(dest, exist_ok=True)
                        shutil.move(chemin_complet, os.path.join(dest, fichier))
                        print(f"âš ï¸ Fichier invalide dÃ©placÃ© : {chemin_complet}")

    print("âš ï¸ Nettoyage terminÃ©")


# --------------------------------------------------
# ğŸ§¹ 1. Evaluation
# --------------------------------------------------

def compter_references_par_classe(total_images, max_refs=10):

    percentage=0.05

    if total_images <= 0:
        return 0

    num_refs = int(total_images * percentage)
    return min(num_refs, max_refs)

def chercher_references_par_classe(class_dir, required_count):

    reference_images = []

    for fname in os.listdir(class_dir):
        if "reference" in fname.lower() and fname.lower().endswith((".jpg", ".jpeg", ".png")):
            reference_images.append(os.path.join(class_dir, fname))

    if len(reference_images) < required_count:
        raise ValueError(
            f"âŒ Classe '{os.path.basename(class_dir)}' : {len(reference_images)} image(s) de rÃ©fÃ©rence trouvÃ©e(s), "
            f"il en faut au moins {required_count}. Ajoutez davantage dâ€™images avec 'reference' dans le nom."
        )
    
    else:
        print(f"âœ… Classe '{os.path.basename(class_dir)}' : {len(reference_images)} image(s) de rÃ©fÃ©rence validÃ©e(s).")
    return reference_images


def extract_feature(image_path, model, transform, device):
    img = Image.open(image_path).convert("RGB")
    img_tensor = transform(img).unsqueeze(0).to(device)
    with torch.no_grad():
        feat = model(img_tensor).cpu().numpy()[0]
    return feat

def evaluer_dataset(dataset_dir, is_illustration, device, classes_paths, mobilenet, img_height, img_width):
    print("\nğŸ“Š Ã‰VALUATION DU DATASET :", dataset_dir)

    # ğŸ”§ Transformation de l'image pour MobileNet :
    # - Redimensionnement (Resize)
    # - Conversion en tenseur (ToTensor)
    # - Normalisation avec les valeurs d'ImageNet

    preprocess = transforms.Compose([
        transforms.Resize((img_height, img_width)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    class_images = defaultdict(list)

    for class_name in classes_paths:
        full_class_path = os.path.join(dataset_dir, class_name)
        print(f"\nğŸ” Analyse du dossier : {class_name}")

        if not os.path.isdir(full_class_path):
            print(f"âš ï¸ Chemin non trouvÃ© : {full_class_path}")
            continue

        all_files = [f for f in os.listdir(full_class_path)
                     if f.lower().endswith((".png", ".jpg", ".jpeg"))]

        # SÃ©lection des rÃ©fÃ©rences
        required_refs = compter_references_par_classe(len(all_files))
        try:
            reference_paths = chercher_references_par_classe(full_class_path, required_refs)
        except ValueError as e:
            print(e)
            continue

        # Extraction des vecteurs de rÃ©fÃ©rence
        reference_features = []
        for ref_path in reference_paths:
            feat = extract_feature(ref_path, mobilenet, preprocess, device)
            reference_features.append(feat)

        reference_features = np.array(reference_features)

        # VÃ©rification homogÃ©nÃ©itÃ© des rÃ©fÃ©rences
        if len(reference_features) >= 2:
            sim_matrix = cosine_similarity(reference_features)
            upper_triangle = sim_matrix[np.triu_indices(len(sim_matrix), k=1)]
            mean_ref_similarity = np.mean(upper_triangle)
            if mean_ref_similarity < 0.7:
                print(f"âš ï¸ RÃ©fÃ©rences trop hÃ©tÃ©rogÃ¨nes (score : {mean_ref_similarity:.2f})")
                continue
            else:
                print(f"âœ… RÃ©fÃ©rences valides (score : {mean_ref_similarity:.2f})")
        else:
            mean_ref_similarity = 1.0

        # Ã‰valuation des autres images (y compris les rÃ©fÃ©rences pour comparaison)
        print(f"\nğŸ“Š Ã‰valuation des images pour la classe : {class_name}")
        for fname in tqdm(all_files, desc="ğŸ” Ã‰valuation"):
            img_path = os.path.join(full_class_path, fname)
            feat = extract_feature(img_path, mobilenet, preprocess, device)

            # HomogÃ©nÃ©itÃ© : moyenne de similaritÃ© avec les rÃ©fÃ©rences
            similarities = cosine_similarity([feat], reference_features)[0]
            mean_similarity = np.mean(similarities)

            # QualitÃ©
            img = Image.open(img_path).convert("RGB")
            img_resized = img.resize((img_height, img_width))
            img_array = np.array(img_resized)
            gray = cv2.cvtColor(img_array.astype(np.uint8), cv2.COLOR_RGB2GRAY)
            quality_score = np.std(gray) if is_illustration else cv2.Laplacian(gray, cv2.CV_64F).var()

            class_images[class_name].append((fname, mean_similarity, quality_score))

    sorted_images_by_class = {}

    # Rapport final par classe
    for class_name, items in class_images.items():
        print(f"\nğŸ“‚ Classe : {class_name} | Nombre dâ€™images : {len(items)}")

        if not items:
            print("âš ï¸ Aucune image Ã  Ã©valuer.")
            continue

        items.sort(key=lambda x: x[1], reverse=True)  # tri par homogÃ©nÃ©itÃ© dÃ©croissante
        sorted_fnames = [fname for fname, _, _ in items]
        sorted_images_by_class[class_name] = sorted_fnames

        # Extraction des similaritÃ©s triÃ©es
        similarities = [sim for _, sim, _ in items]

        # Calcul du point de rupture amÃ©liorÃ©
        max_composite_gap = -1
        best_idx = -1

        for i in range(1, len(similarities) - 1):  # on Ã©vite les extrÃ©mitÃ©s
            delta_before = abs(similarities[i] - similarities[i - 1])
            delta_after = abs(similarities[i] - similarities[i + 1])
            composite_gap = delta_before + delta_after

            if composite_gap > max_composite_gap:
                max_composite_gap = composite_gap
                best_idx = i

        if best_idx != -1:
            sim_before = similarities[best_idx - 1]
            sim_target = similarities[best_idx]
            sim_after = similarities[best_idx + 1]

            discriminability_threshold = (sim_before + sim_after) / 2

            print(f"\nğŸ“Œ Point de discriminabilitÃ© trouvÃ© Ã  : {discriminability_threshold:.4f}")
            print(f"ğŸ§­ Point de rupture : image au centre = '{items[best_idx][0]}'")
            print(f"   â†³ Î” avant = {abs(sim_target - sim_before):.4f}, Î” aprÃ¨s = {abs(sim_target - sim_after):.4f}, "
                f"Î” total = {max_composite_gap:.4f}")

            # Affichage des Ã©carts combinÃ©s pour chaque image (sauf extrÃ©mitÃ©s)
            delta_summary = []
            print("\nğŸ“‰ Ã‰carts combinÃ©s (avant + aprÃ¨s) pour chaque image :")
            for i in range(len(similarities)):
                fname = items[i][0]
                if i == 0:
                    delta = abs(similarities[i] - similarities[i + 1])
                    delta_summary.append((fname, delta, None, delta))  # (nom, Î” aprÃ¨s, Î” avant, Î” total)

                elif i == len(similarities) - 1:
                    delta = abs(similarities[i] - similarities[i - 1])
                    delta_summary.append((fname, None, delta, delta))
                else:
                    delta_before = abs(similarities[i] - similarities[i - 1])
                    delta_after = abs(similarities[i] - similarities[i + 1])
                    delta_total = delta_before + delta_after
                    delta_summary.append((fname, delta_after, delta_before, delta_total))
            # Tri dÃ©croissant selon Î” total
            delta_summary.sort(key=lambda x: x[3], reverse=True)

            # Affichage triÃ©
            print("\nğŸ“‰ Ã‰carts combinÃ©s triÃ©s (Î” total dÃ©croissant) :")
            for fname, delta_after, delta_before, delta_total in delta_summary:
                if delta_before is None:
                    print(f" - {fname} (tÃªte)       : Î” aprÃ¨s = {delta_after:.4f} | Î” total = {delta_total:.4f}")
                elif delta_after is None:
                    print(f" - {fname} (queue)      : Î” avant = {delta_before:.4f} | Î” total = {delta_total:.4f}")
                else:
                    print(f" - {fname}              : Î” avant = {delta_before:.4f} | Î” aprÃ¨s = {delta_after:.4f} | Î” total = {delta_total:.4f}")
        else:
            print("\nâš ï¸ Pas assez de donnÃ©es pour calculer un point de discriminabilitÃ©.")


            # Images proches du seuil (Â±0.05)
            #border_images = [(fname, sim, qual) for fname, sim, qual in items
                            #if abs(sim - discriminability_threshold) < 0.05]

            #print("\nâš ï¸ Images proches de la frontiÃ¨re discriminative :")
            #for fname, sim, qual in border_images:
                #print(f" - {fname} | HomogÃ©nÃ©itÃ© : {sim:.4f} | QualitÃ© : {qual:.2f}")
        #else:
            #print("\nğŸ“Œ Pas assez d'images pour calculer un point de discriminabilitÃ©.")


        for fname, sim, qual in items:
            sim_flag = "ğŸ”´ Mauvais" if sim < 0.5 else "ğŸŸ¡ Moyen" if sim < 0.7 else "ğŸŸ¢ Bon" if sim < 0.9 else "ğŸ”µ Excellent"
            qual_flag = "ğŸ”´ Mauvais" if qual < 30 else "ğŸŸ¡ Moyen" if qual < 50 else "ğŸŸ¢ Bon" if qual < 100 else "ğŸ”µ Excellent"
            is_reference = "â­" if os.path.join(dataset_dir, class_name, fname) in reference_paths else " "
            print(f"ğŸ”¹ {fname} {is_reference} | HomogÃ©nÃ©itÃ© : {sim:.4f} ({sim_flag}) | QualitÃ© : {qual:.2f} ({qual_flag})")
            #print(f"ğŸ”¹ {fname} | HomogÃ©nÃ©itÃ© : {sim:.4f} ({sim_flag}) | QualitÃ© : {qual:.2f} ({qual_flag})")

        mean_sim = np.mean([sim for _, sim, _ in items])
        mean_qual = np.mean([qual for _, _, qual in items])
        print(f"\nğŸ“Š Moyenne homogÃ©nÃ©itÃ© : {mean_sim:.4f}")
        print(f"ğŸ“Š Moyenne qualitÃ©     : {mean_qual:.2f}")
        return sorted_images_by_class


def augmenter_dataset(base_path, image_type, augment_path, classes_folders, seed, img_height, img_width, images_par_homogeneite=None):
    print("\nğŸ”„ AUGMENTATION DES DONNÃ‰ES...")

    # Copier tout le dataset original dans le dossier de sortie
    if os.path.exists(augment_path):
        shutil.rmtree(augment_path)  # Supprime l'ancien dossier s'il existe pour Ã©viter les doublons

    os.makedirs(augment_path, exist_ok=True)

    # ğŸ”¹ Copier uniquement les dossiers autorisÃ©s
    for class_name in classes_folders:
        source_dir = os.path.join(base_path, class_name)
        dest_dir = os.path.join(augment_path, class_name)

        if os.path.exists(source_dir):
            shutil.copytree(source_dir, dest_dir)
        else:
            print(f"âš ï¸ Dossier introuvable : {source_dir}")

    random.seed(seed)
    torch.manual_seed(seed)

    # 1. DÃ©finir les transformations
    if image_type == "illustration":
        augmentation = transforms.Compose([
            transforms.RandomRotation(degrees=5),
            transforms.RandomResizedCrop(size=img_height, scale=(0.9, 1.0)),
            transforms.RandomApply([transforms.ColorJitter(contrast=0.1)], p=0.7),
            transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),
        ])
    else:
        augmentation = transforms.Compose([
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(10),
            transforms.RandomResizedCrop(size=img_height, scale=(0.9, 1.0)),
            transforms.ColorJitter(contrast=0.1)
        ])

    # 2. Rassembler les images par classe
    class_images = defaultdict(list)

    for class_name in classes_folders:
        class_path = os.path.join(augment_path, class_name)
        if not os.path.isdir(class_path):
            continue

        print(f"\nğŸ” Traitement de la classe : {class_name}")

        top_selected = []
        if images_par_homogeneite:
            top_imgs = images_par_homogeneite.get(class_name, [])
            cutoff = int(len(top_imgs) * 2 / 40)
            top_selected = top_imgs[:cutoff]

            print(f"ğŸ† Top 2/3 des images les plus homogÃ¨nes ({len(top_selected)} images) :")
            for img in top_selected:
                print(f"   - {img}")

        for fname in os.listdir(class_path):
            if fname.lower().endswith((".png", ".jpg", ".jpeg")):
                full_path = os.path.join(class_path, fname)

                if images_par_homogeneite:
                    if fname not in top_selected:
                        print(f"â›” IgnorÃ©e (hors top 2/3) : {fname}")
                        continue
                    print(f"âœ… GardÃ©e (top 2/3) : {fname}")

                class_images[class_name].append(full_path)

    # 3. Calculer combien dâ€™images ajouter par classe
    # On garde la trace du nombre total dâ€™images originales pour chaque classe
    true_class_counts = {class_name: len(images_par_homogeneite.get(class_name, [])) for class_name in classes_folders}
    max_count = max(true_class_counts.values())

    for label, image_paths in class_images.items():
        current_total = true_class_counts.get(label, len(image_paths))  # total images originales
        current_filtered = len(image_paths)  # uniquement les top 2/3 retenues

        if current_total == max_count:
            # Si câ€™est la classe la plus reprÃ©sentÃ©e, on augmente de 5%
            extra_needed = max(1, int(current_total * 0.05))
        else:
            # Sinon, on Ã©galise jusquâ€™Ã  la classe majoritaire
            extra_needed = max_count - current_total

        print(f"ğŸ”§ {label} â†’ Ajout de {extra_needed} images (sur {current_filtered} images sÃ©lectionnÃ©es)")

        save_dir = os.path.join(augment_path, label)
        os.makedirs(save_dir, exist_ok=True)

        # 4. GÃ©nÃ©rer des images augmentÃ©es
        for i in tqdm(range(extra_needed), desc=""):
            path = random.choice(image_paths)
            image = Image.open(path).convert("RGB").resize((img_height, img_width))
            aug_image = augmentation(image)

            save_path = os.path.join(save_dir, f"aug_{i}.jpg")
            aug_image.save(save_path)

            print(f"ğŸ“ˆ Augmentation â†’ Image source : {os.path.basename(path)} â†’ Fichier gÃ©nÃ©rÃ© : {os.path.basename(save_path)}")